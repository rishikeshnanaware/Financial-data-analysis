# -*- coding: utf-8 -*-
"""SambhajiMaharaj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NznvHTMkUyHyI2IzH_FNaxAqilb5j_yu
"""

!pip install kaggle #Installing Kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sbhatti/financial-sentiment-analysis #Downloading Kaggle dataset

from zipfile import ZipFile
dataset = '/content/financial-sentiment-analysis.zip'
with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('Done')

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

finance_data=pd.read_csv('/content/data.csv', encoding='ISO-8859-1') #Reading dataset from data file and storing in a dataframe

finance_data.head() #Printing first five rows of dataset

finance_data.shape #Dataframe has 5842 rows and 2 columns

finance_data.rename(columns = {'Sentiment':'target'}, inplace=True) #Renaming column name 'Sentiment' as 'target'

finance_data.head()

finance_data.replace({'target':{'positive':1}},inplace=True) #Replacing value 'positive' in column 'target' to 1

finance_data.replace({'target':{'neutral':0}},inplace=True) #Replacing value 'neutral' in column 'target' to 0
finance_data.replace({'target':{'negative':-1}},inplace=True) #Replacing value 'negative' in column 'target' to -1

finance_data.head()

finance_data['target'].value_counts() #Displaying total positive, neutral and negative sentiments in dataset

finance_data_neutral=finance_data[finance_data['target']==0]

finance_data_neutral.head()

finance_data_positive=finance_data[finance_data['target']==1]

drop_indices = np.random.choice(finance_data_neutral.index, 2270, replace=False)
finance_data_neutral.drop(drop_indices, inplace=True)
finance_data_neutral.head()

finance_data_neutral.shape

drop_indices = np.random.choice(finance_data_positive.index, 992, replace=False)
finance_data_positive.drop(drop_indices, inplace=True)
finance_data_positive.head()
finance_data_positive.shape

finance_data=finance_data[finance_data.target == -1]
finance_data.head()

finance_data=pd.concat([finance_data,finance_data_neutral],ignore_index=False)

finance_data.sort_index()

finance_data=pd.concat([finance_data,finance_data_positive],ignore_index=False)

finance_data.sort_index(inplace=True)

finance_data.head(10)

finance_data.head(10)

port_stem=PorterStemmer()

def stemming(content):                                                                         # Making a stemmer to stem Sentences
  stemmed_content=re.sub('[^a-zA-Z]', ' ', content)
  stemmed_content=stemmed_content.lower()
  stemmed_content=stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content=' '.join(stemmed_content)
  return stemmed_content

finance_data['stemmed_content']=finance_data['Sentence'].apply(stemming)                       #Passing dataframe to stemmer

print(finance_data['stemmed_content'])                                                          # Displaying stemmed content

x=finance_data['stemmed_content'].values
y=finance_data['target'].values
print(x)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)                   #Splitting data as training data and testing data

finance_data['stemmed_content'].isnull().sum()

print(x_train)

vectorizer=TfidfVectorizer()
x_train=vectorizer.fit_transform(x_train)                                            # Extracting features from training data
x_test=vectorizer.transform(x_test)                                                  # Extracting features from testing data

mymodel=LogisticRegression(max_iter=1000)                                            # Using LogisticRegressing model

mymodel.fit(x_train,y_train)                                                         # Training model

x_train_prediction=mymodel.predict(x_train)                                           # Predicting trained data values
training_data_accuracy=accuracy_score(y_train,x_train_prediction)                     # Accuracy on trained data

print("Accuracy = ",training_data_accuracy)

x_test_prediction=mymodel.predict(x_test)                                            # Predicting testing data values
test_data_accuracy=accuracy_score(y_test,x_test_prediction)                          # Accuracy on trained data

print("Accuracy = ",test_data_accuracy)

"""# New Section"""

!pip install kaggle  # Installing Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d sbhatti/financial-sentiment-analysis  # Downloading Kaggle dataset

from zipfile import ZipFile
dataset = '/content/financial-sentiment-analysis.zip'

# Extract dataset
with ZipFile(dataset, 'r') as zip:
    zip.extractall()
    print('Done')

# Import libraries
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  # Support Vector Classifier
from sklearn.ensemble import RandomForestClassifier  # Random Forest
from sklearn.metrics import accuracy_score
import nltk

# Download stopwords
nltk.download('stopwords')

# Read dataset
finance_data = pd.read_csv('/content/data.csv', encoding='ISO-8859-1')
finance_data.rename(columns={'Sentiment': 'target'}, inplace=True)

# Replace sentiments with numeric values
finance_data.replace({'target': {'positive': 1, 'neutral': 0, 'negative': -1}}, inplace=True)

# Balance the dataset by randomly dropping extra instances of neutral and positive sentiments
finance_data_neutral = finance_data[finance_data['target'] == 0]
finance_data_positive = finance_data[finance_data['target'] == 1]

# Downsampling neutral and positive sentiments to match negative sentiment count
drop_indices_neutral = np.random.choice(finance_data_neutral.index, 2270, replace=False)
finance_data_neutral.drop(drop_indices_neutral, inplace=True)

drop_indices_positive = np.random.choice(finance_data_positive.index, 992, replace=False)
finance_data_positive.drop(drop_indices_positive, inplace=True)

# Concatenate balanced dataset
finance_data = pd.concat([finance_data[finance_data['target'] == -1], finance_data_neutral, finance_data_positive], ignore_index=True)

# Text preprocessing: Stemming
port_stem = PorterStemmer()

def stemming(content):
    content = re.sub('[^a-zA-Z]', ' ', content).lower().split()
    return ' '.join([port_stem.stem(word) for word in content if word not in stopwords.words('english')])

finance_data['stemmed_content'] = finance_data['Sentence'].apply(stemming)

# Split dataset into features and target
x = finance_data['stemmed_content'].values
y = finance_data['target'].values

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)

# Vectorization
vectorizer = TfidfVectorizer()
x_train = vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Support Vector Machine': SVC(),
    'Random Forest': RandomForestClassifier()
}

# Train models and evaluate accuracy
for model_name, model in models.items():
    model.fit(x_train, y_train)
    train_pred = model.predict(x_train)
    test_pred = model.predict(x_test)

    train_accuracy = accuracy_score(y_train, train_pred)
    test_accuracy = accuracy_score(y_test, test_pred)

    print(f"{model_name} - Training Accuracy: {train_accuracy}")
    print(f"{model_name} - Test Accuracy: {test_accuracy}")

# Installing necessary libraries
!pip install kaggle

# Importing required libraries
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  # Support Vector Classifier
from sklearn.ensemble import RandomForestClassifier  # Random Forest
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import nltk
from zipfile import ZipFile
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# Download stopwords
nltk.download('stopwords')
nltk.download('wordnet')

# Install Kaggle and authenticate
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset from Kaggle
!kaggle datasets download -d sbhatti/financial-sentiment-analysis  # Downloading Kaggle dataset

# Extract dataset
dataset = '/content/financial-sentiment-analysis.zip'

with ZipFile(dataset, 'r') as zip:
    zip.extractall()
    print('Done')

# Read dataset
finance_data = pd.read_csv('/content/data.csv', encoding='ISO-8859-1')

# Replace sentiments with numeric values
finance_data.replace({'Sentiment': {'positive': 1, 'neutral': 0, 'negative': -1}}, inplace=True)

finance_data.rename(columns = {'Sentiment':'target'}, inplace=True) #Renaming column name 'Sentiment' as 'target'

# Shift the target values so that they are in the range [0, 1, 2]
finance_data['target'] = finance_data['target'] + 1  # Convert -1, 0, 1 to 0, 1, 2

# Balance the dataset by randomly dropping extra instances of neutral and positive sentiments
finance_data_neutral = finance_data[finance_data['target'] == 1]
finance_data_positive = finance_data[finance_data['target'] == 2]

# Downsampling neutral and positive sentiments to match negative sentiment count
drop_indices_neutral = np.random.choice(finance_data_neutral.index, 2270, replace=False)
finance_data_neutral.drop(drop_indices_neutral, inplace=True)

drop_indices_positive = np.random.choice(finance_data_positive.index, 992, replace=False)
finance_data_positive.drop(drop_indices_positive, inplace=True)

# Concatenate balanced dataset
finance_data = pd.concat([finance_data[finance_data['target'] == 0], finance_data_neutral, finance_data_positive], ignore_index=True)

# Text preprocessing: Stemming
port_stem = PorterStemmer()

def stemming(content):
    content = re.sub('[^a-zA-Z]', ' ', content).lower().split()
    return ' '.join([port_stem.stem(word) for word in content if word not in stopwords.words('english')])

finance_data['stemmed_content'] = finance_data['Sentence'].apply(stemming)

# Split dataset into features and target
x = finance_data['stemmed_content'].values
y = finance_data['target'].values

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)

# Vectorization
vectorizer = TfidfVectorizer()
x_train = vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Support Vector Machine': SVC(probability=True),  # Enable probability estimates for SVM
    'Random Forest': RandomForestClassifier(),
    'XGBoost': xgb.XGBClassifier(objective='multi:softmax', num_class=3)  # XGBoost model with 3 classes
}

# Hyperparameter tuning for SVM using GridSearchCV
svm_params = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto']
}

svm_grid_search = GridSearchCV(SVC(probability=True), svm_params, cv=5)  # Enable probability=True for GridSearch
svm_grid_search.fit(x_train, y_train)
best_svm_params = svm_grid_search.best_params_
print(f"Best SVM parameters: {best_svm_params}")

# Train models and evaluate accuracy
for model_name, model in models.items():
    if model_name == 'Support Vector Machine':
        model = svm_grid_search.best_estimator_  # Use best parameters for SVM

    model.fit(x_train, y_train)
    train_pred = model.predict(x_train)
    test_pred = model.predict(x_test)

    train_accuracy = accuracy_score(y_train, train_pred)
    test_accuracy = accuracy_score(y_test, test_pred)

    print(f"{model_name} - Training Accuracy: {train_accuracy}")
    print(f"{model_name} - Test Accuracy: {test_accuracy}")

    # Classification Report
    print(f"{model_name} - Classification Report:\n", classification_report(y_test, test_pred))

    # AUC-ROC Score
    if hasattr(model, "predict_proba"):  # Check if model has predict_proba method
        auc_roc = roc_auc_score(y_test, model.predict_proba(x_test), multi_class='ovr')
        print(f"{model_name} - AUC-ROC Score: {auc_roc}\n")
    else:
        print(f"{model_name} - AUC-ROC Score: Not available (no probability estimates)\n")